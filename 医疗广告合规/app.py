import streamlit as st
import json
from pathlib import Path
from datetime import datetime, timezone
from PIL import Image
import tempfile

# åœ¨æœ€å¼€å§‹ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
@st.cache_resource
def get_semantic_model_cached():
    """ç¼“å­˜è¯­ä¹‰æ¨¡å‹ï¼Œä»…åŠ è½½ä¸€æ¬¡ã€‚"""
    from sentence_transformers import SentenceTransformer
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')


@st.cache_resource
def get_classifier_cached():
    """ç¼“å­˜åˆ†ç±»å™¨æ¨¡å‹ï¼Œä»…åŠ è½½ä¸€æ¬¡ã€‚"""
    try:
        from transformers import BertTokenizerFast, BertForSequenceClassification
        import torch
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        tokenizer = BertTokenizerFast.from_pretrained('models/violation_classifier')
        model = BertForSequenceClassification.from_pretrained('models/violation_classifier')
        model.to(device)
        model.eval()
        return {'model': model, 'tokenizer': tokenizer, 'device': device}
    except Exception:
        return None


@st.cache_data
def load_rules_cached():
    """ç¼“å­˜è§„åˆ™æ–‡ä»¶åŠ è½½ã€‚"""
    from utils.text_processing import load_sensitive_words, load_violation_rules
    return {
        'sensitive': load_sensitive_words('sensitive_words.txt'),
        'violation': load_violation_rules('violation_rules.txt')
    }


def highlight_text(text, matches):
    """å°†åŒ¹é…çš„ä½ç½®æ ‡çº¢ã€‚
    
    åŒ¹é…æ ¼å¼ï¼š[{'span': [start, end], 'matched_text': '...', ...}, ...]
    """
    if not matches:
        return text
    
    # æŒ‰èµ·å§‹ä½ç½®å€’åºæ’åˆ—ï¼Œä»åå¾€å‰æ›¿æ¢ä»¥é¿å…ä½ç½®åç§»
    sorted_matches = sorted(matches, key=lambda x: x['span'][0], reverse=True)
    
    result = text
    for match in sorted_matches:
        start, end = match['span']
        matched_text = match['matched_text']
        # ç”¨çº¢è‰²é«˜äº®æ›¿æ¢
        highlighted = f'<span style="background-color: #ffcccc; color: red; font-weight: bold;">{matched_text}</span>'
        result = result[:start] + highlighted + result[end:]
    
    return result


# å¯¼å…¥å…¶ä»–æ¨¡å—
from utils.ocr import ocr_from_image
from utils.text_processing import tokenize, regex_matches, violation_matches
from utils.database import init_db, save_report, get_reports, get_report_by_id, delete_report, get_statistics
import config

# ============ åˆå§‹åŒ– ============
st.set_page_config(page_title='åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ', layout='wide', initial_sidebar_state='expanded')

@st.cache_resource
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä»…ä¸€æ¬¡ï¼‰ã€‚"""
    init_db()
    return True

init_database()

st.title('ğŸ¥ åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ')
st.markdown('---')

# ============ ä¾§è¾¹æ  ============
page = st.sidebar.radio('é€‰æ‹©åŠŸèƒ½', ('æ£€æµ‹', 'æ‰¹é‡æ£€æµ‹', 'å†å²æŠ¥å‘Š', 'è§„åˆ™ç®¡ç†'))

with st.sidebar:
    if page != 'è§„åˆ™ç®¡ç†':
        st.header('âš™ï¸ æ£€æµ‹é…ç½®')
        semantic_threshold = st.slider('è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼', 0.0, 1.0, config.SEMANTIC_THRESHOLD, 0.01)
        enable_classifier = st.checkbox('å¯ç”¨å¾®è°ƒåˆ†ç±»å™¨', value=False)  # é»˜è®¤å…³é—­ä»¥åŠ é€Ÿ
        enable_violation_rules = st.checkbox('å¯ç”¨è¿è§„è§„åˆ™åŒ¹é…', value=True)
        enable_semantic = st.checkbox('å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹', value=False)  # é»˜è®¤å…³é—­ä»¥åŠ é€Ÿ
    
    st.markdown('---')
    st.markdown('â±ï¸ **æ€§èƒ½ä¼˜åŒ–**')
    st.caption('ä¸ºäº†åŠ é€Ÿé¦–æ¬¡åŠ è½½ï¼Œå·²é»˜è®¤å…³é—­'
               'è¯­ä¹‰æ£€æµ‹å’Œåˆ†ç±»å™¨ã€‚è¯·åœ¨éœ€è¦æ—¶å¯ç”¨ã€‚')


# ============ æ£€æµ‹é¡µé¢ ============
if page == 'æ£€æµ‹':
    detection_mode = st.radio('é€‰æ‹©æ£€æµ‹ç±»å‹', ('æ–‡æœ¬æ£€æµ‹', 'å›¾ç‰‡OCRæ£€æµ‹'))
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader('ğŸ“ è¾“å…¥')
        
        if detection_mode == 'æ–‡æœ¬æ£€æµ‹':
            input_text = st.text_area('è¯·è¾“å…¥è¦æ£€æµ‹çš„æ–‡æœ¬', height=300, placeholder='è¾“å…¥åŒ»ç–—å¹¿å‘Šæ–‡æ¡ˆ...')
            ocr_text = ''
        else:
            st.markdown('**ä¸Šä¼ å›¾ç‰‡è¿›è¡Œ OCR è¯†åˆ«**')
            uploaded_file = st.file_uploader('é€‰æ‹©å›¾ç‰‡æ–‡ä»¶', type=['jpg', 'jpeg', 'png', 'bmp'])
            input_text = ''
            ocr_text = ''
            
            if uploaded_file is not None:
                image = Image.open(uploaded_file)
                st.image(image, caption='ä¸Šä¼ çš„å›¾ç‰‡', use_column_width=True)
                
                if st.button('ğŸ” è¿è¡Œ OCR è¯†åˆ«', key='ocr_btn'):
                    with st.spinner('æ­£åœ¨è¿›è¡Œ OCR è¯†åˆ«...'):
                        try:
                            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                                tmp.write(uploaded_file.getbuffer())
                                tmp_path = tmp.name
                            ocr_text = ocr_from_image(tmp_path)
                            input_text = ocr_text
                            st.success('âœ… OCR è¯†åˆ«å®Œæˆ')
                            st.text_area('OCR è¯†åˆ«ç»“æœ', value=ocr_text, height=200, disabled=True)
                        except Exception as e:
                            st.error(f'âŒ OCR è¯†åˆ«å¤±è´¥: {e}')

    with col2:
        st.subheader('ğŸ“Š æ£€æµ‹ç»“æœ')
        
        if detection_mode == 'æ–‡æœ¬æ£€æµ‹' and input_text:
            perform_detection = st.button('ğŸ” è¿è¡Œæ£€æµ‹', key='text_detect_btn')
        elif detection_mode == 'å›¾ç‰‡OCRæ£€æµ‹' and input_text:
            perform_detection = st.button('ğŸ” è¿è¡Œæ£€æµ‹', key='image_detect_btn')
        else:
            perform_detection = False
        
        if perform_detection and input_text:
            with st.spinner('æ­£åœ¨æ£€æµ‹...'):
                # 1. åˆ†è¯
                st.info('ğŸ“Š æ‰§è¡Œåˆ†è¯...')
                tokens = tokenize(input_text)
                
                # 2. åŠ è½½è§„åˆ™
                rules_data = load_rules_cached()
                
                # 3. æ•æ„Ÿè¯æ­£åˆ™åŒ¹é…
                st.info('ğŸ” æ‰§è¡Œæ•æ„Ÿè¯åŒ¹é…...')
                regex_res = regex_matches(input_text, rules_data['sensitive'], flags=config.REGEX_FLAGS)
                
                # 4. è¿è§„è§„åˆ™åŒ¹é…
                violation_res = []
                if enable_violation_rules:
                    st.info('ğŸ“‹ æ‰§è¡Œè¿è§„è§„åˆ™åŒ¹é…...')
                    violation_res = violation_matches(input_text, rules=rules_data['violation'])
                
                # 5. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹
                semantic_res = []
                if enable_semantic:
                    st.info('â³ åŠ è½½è¯­ä¹‰æ¨¡å‹ä¸­ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼‰...')
                    try:
                        model = get_semantic_model_cached()
                        st.info('ğŸ”„ æ‰§è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹...')
                        from numpy import argmax
                        from sklearn.metrics.pairwise import cosine_similarity
                        texts = [input_text] + rules_data['sensitive']
                        embs = model.encode(texts, convert_to_numpy=True)
                        sims = cosine_similarity(embs[0:1], embs[1:])[0]
                        for i, s in enumerate(sims):
                            if s >= semantic_threshold:
                                semantic_res.append({'example': rules_data['sensitive'][i], 'score': float(s)})
                        semantic_res = sorted(semantic_res, key=lambda x: x['score'], reverse=True)[:3]
                    except Exception as e:
                        st.warning(f'âš ï¸ è¯­ä¹‰æ£€æµ‹å¤±è´¥: {e}')
                
                # 6. åˆ†ç±»å™¨é¢„æµ‹
                classifier_res = None
                if enable_classifier:
                    st.info('â³ åŠ è½½åˆ†ç±»å™¨ä¸­ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼‰...')
                    try:
                        import torch
                        import numpy as np
                        clf = get_classifier_cached()
                        if clf:
                            tokenizer, model, device = clf['tokenizer'], clf['model'], clf['device']
                            inputs = tokenizer(input_text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')
                            inputs = {k: v.to(device) for k, v in inputs.items()}
                            with torch.no_grad():
                                outputs = model(**inputs)
                                logits = outputs.logits.cpu().numpy()[0]
                                e_x = np.exp(logits - np.max(logits))
                                probs = e_x / e_x.sum()
                                label = int(argmax(probs))
                                classifier_res = {'label': label, 'score': float(probs[label]), 'probs': [float(p) for p in probs]}
                    except Exception as e:
                        st.warning(f'âš ï¸ åˆ†ç±»å™¨åŠ è½½å¤±è´¥: {e}')
                
                # 7. ç»¼åˆåˆ¤å®š
                has_violations = len(violation_res) > 0 or len(regex_res) > 0 or len(semantic_res) > 0
                if classifier_res and classifier_res.get('label') == 1:
                    has_violations = True
                
                verdict = 'åˆè§„' if not has_violations else 'ç–‘ä¼¼è¿è§„'
                verdict_display = 'ğŸŸ¢ åˆè§„' if not has_violations else 'ğŸ”´ ç–‘ä¼¼è¿è§„'
                
                st.markdown(f"### ç»¼åˆåˆ¤å®š: {verdict_display}")
                
                # ç»“æœæ ‡ç­¾é¡µ
                tab1, tab2, tab3, tab4, tab5 = st.tabs(['è¿è§„è§„åˆ™åŒ¹é…', 'æ•æ„Ÿè¯åŒ¹é…', 'è¯­ä¹‰ç›¸ä¼¼åº¦', 'åˆ†ç±»å™¨', 'åˆ†è¯ç»“æœ'])
                
                with tab1:
                    if enable_violation_rules and violation_res:
                        st.markdown(f"**å‘ç° {len(violation_res)} ä¸ªè¿è§„è§„åˆ™åŒ¹é…**")
                        st.markdown('---')
                        st.markdown('**æ£€æµ‹æ–‡æœ¬ï¼ˆçº¢è‰²æ ‡è®°ä¸ºé—®é¢˜ä½ç½®ï¼‰ï¼š**')
                        highlighted = highlight_text(input_text, violation_res)
                        st.markdown(highlighted, unsafe_allow_html=True)
                        st.markdown('---')
                        st.markdown('**è¯¦ç»†åŒ¹é…ï¼š**')
                        for idx, match in enumerate(violation_res, 1):
                            st.write(f"**{idx}. è¿è§„è¯**: `{match['rule']}`")
                            st.divider()
                    else:
                        st.info('âœ… æœªæ£€æµ‹åˆ°è¿è§„è§„åˆ™åŒ¹é…')
                
                with tab2:
                    if regex_res:
                        st.markdown(f"**å‘ç° {len(regex_res)} ä¸ªæ•æ„Ÿè¯åŒ¹é…**")
                        st.markdown('---')
                        st.markdown('**æ£€æµ‹æ–‡æœ¬ï¼ˆçº¢è‰²æ ‡è®°ä¸ºé—®é¢˜ä½ç½®ï¼‰ï¼š**')
                        highlighted = highlight_text(input_text, regex_res)
                        st.markdown(highlighted, unsafe_allow_html=True)
                        st.markdown('---')
                        st.markdown('**è¯¦ç»†åŒ¹é…ï¼š**')
                        for idx, match in enumerate(regex_res, 1):
                            st.write(f"**{idx}. æ•æ„Ÿè¯**: `{match['word']}`")
                            st.divider()
                    else:
                        st.info('âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯')
                
                with tab3:
                    if enable_semantic and semantic_res:
                        st.markdown(f"**å‘ç° {len(semantic_res)} ä¸ªç›¸ä¼¼åº¦é«˜çš„ç¤ºä¾‹**")
                        for idx, match in enumerate(semantic_res, 1):
                            st.write(f"**{idx}. ç¤ºä¾‹**: `{match['example']}`")
                            st.write(f"   **ç›¸ä¼¼åº¦åˆ†æ•°**: {match['score']:.4f}")
                            st.divider()
                    elif enable_semantic:
                        st.info('âœ… æœªæ£€æµ‹åˆ°é«˜ç›¸ä¼¼åº¦ç¤ºä¾‹')
                    else:
                        st.info('â­ï¸ è¯­ä¹‰æ£€æµ‹å·²ç¦ç”¨')
                
                with tab4:
                    if classifier_res:
                        label_text = 'ç–‘ä¼¼è¿è§„ (label=1)' if classifier_res['label'] == 1 else 'åˆè§„ (label=0)'
                        st.write(f"**åˆ†ç±»ç»“æœ**: {label_text}")
                        st.write(f"**ç½®ä¿¡åº¦**: {classifier_res['score']:.4f}")
                        col_prob1, col_prob2 = st.columns(2)
                        with col_prob1:
                            st.metric('åˆè§„æ¦‚ç‡', f"{classifier_res['probs'][0]:.4f}")
                        with col_prob2:
                            st.metric('è¿è§„æ¦‚ç‡', f"{classifier_res['probs'][1]:.4f}")
                    else:
                        st.info('â­ï¸ åˆ†ç±»å™¨æœªå¯ç”¨æˆ–åŠ è½½å¤±è´¥')
                
                with tab5:
                    st.write('**åˆ†è¯ç»“æœ**')
                    tokens_str = ' / '.join(tokens)
                    st.text(tokens_str)
                    st.write(f'**æ€»è¯æ•°**: {len(tokens)}')
                
                # ç”ŸæˆæŠ¥å‘Š
                st.markdown('---')
                report_data = {
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'input_text': input_text,
                    'ocr_text': ocr_text,
                    'tokens': tokens,
                    'regex_matches': regex_res,
                    'violation_rule_matches': violation_res,
                    'semantic_matches': semantic_res,
                    'classifier': classifier_res,
                    'verdict': verdict
                }
                
                col_save, col_download = st.columns(2)
                with col_save:
                    if st.button('ğŸ’¾ ä¿å­˜åˆ°å†å²'):
                        save_report(report_data)
                        st.success('âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°æ•°æ®åº“')
                
                with col_download:
                    report_json = json.dumps(report_data, ensure_ascii=False, indent=2)
                    st.download_button(
                        label='ğŸ“¥ ä¸‹è½½æŠ¥å‘Š (JSON)',
                        data=report_json,
                        file_name=f'report_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}.json',
                        mime='application/json'
                    )

# ============ æ‰¹é‡æ£€æµ‹é¡µé¢ ============
elif page == 'æ‰¹é‡æ£€æµ‹':
    st.subheader('ğŸ“¦ æ‰¹é‡æ£€æµ‹')
    st.markdown('**ä¸Šä¼ åŒ…å«æ–‡æœ¬çš„æ–‡ä»¶è¿›è¡Œæ‰¹é‡æ£€æµ‹**')
    
    uploaded_file = st.file_uploader('é€‰æ‹©æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰', type=['txt', 'csv'])
    
    if uploaded_file is not None:
        try:
            content = uploaded_file.read().decode('utf-8')
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            st.info(f'ğŸ“Š æ£€æµ‹åˆ° {len(lines)} æ¡æ–‡æœ¬')
            
            if st.button('ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹'):
                results = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                rules_data = load_rules_cached()
                
                for idx, text in enumerate(lines):
                    status_text.text(f'å¤„ç†ä¸­ {idx + 1}/{len(lines)}...')
                    
                    try:
                        regex_res = regex_matches(text, rules_data['sensitive'])
                        violation_res = []
                        if enable_violation_rules:
                            violation_res = violation_matches(text, rules=rules_data['violation'])
                        
                        has_violations = len(violation_res) > 0 or len(regex_res) > 0
                        verdict = 'ç–‘ä¼¼è¿è§„' if has_violations else 'åˆè§„'
                        
                        results.append({
                            'text': text[:100],
                            'verdict': verdict,
                            'violation_count': len(violation_res) + len(regex_res)
                        })
                    except Exception:
                        results.append({
                            'text': text[:100],
                            'verdict': 'é”™è¯¯',
                            'violation_count': 0
                        })
                    
                    progress_bar.progress((idx + 1) / len(lines))
                
                status_text.empty()
                st.success(f'âœ… æ‰¹é‡æ£€æµ‹å®Œæˆ')
                
                st.subheader('æ£€æµ‹ç»“æœ')
                st.dataframe(results)
                
                compliant = sum(1 for r in results if r['verdict'] == 'åˆè§„')
                violations = sum(1 for r in results if r['verdict'] == 'ç–‘ä¼¼è¿è§„')
                
                col1, col2, col3 = st.columns(3)
                col1.metric('æ€»æ•°', len(results))
                col2.metric('åˆè§„', compliant)
                col3.metric('è¿è§„', violations)
                
                result_json = json.dumps(results, ensure_ascii=False, indent=2)
                st.download_button(
                    label='ğŸ“¥ ä¸‹è½½æ‰¹é‡ç»“æœ (JSON)',
                    data=result_json,
                    file_name=f'batch_results_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}.json',
                    mime='application/json'
                )
        except Exception as e:
            st.error(f'âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}')

# ============ å†å²æŠ¥å‘Šé¡µé¢ ============
elif page == 'å†å²æŠ¥å‘Š':
    st.subheader('ğŸ“œ æ£€æµ‹å†å²')
    
    stats = get_statistics()
    col1, col2, col3 = st.columns(3)
    col1.metric('æ€»æ£€æµ‹æ•°', stats['total'])
    col2.metric('è¿è§„æ•°', stats['violations'])
    col3.metric('åˆè§„æ•°', stats['compliant'])
    
    st.markdown('---')
    
    reports = get_reports(limit=50)
    if reports:
        st.markdown(f'**æ˜¾ç¤ºæœ€è¿‘ {len(reports)} æ¡è®°å½•**')
        
        for report_id, timestamp, input_text, verdict, violation_count, created_at in reports:
            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            
            with col1:
                verdict_icon = 'ğŸŸ¢' if verdict == 'åˆè§„' else 'ğŸ”´'
                st.write(f"{verdict_icon} {input_text[:60]}...")
            
            with col2:
                st.write(f"**{verdict}**")
            
            with col3:
                st.write(f"**è¿è§„æ•°**: {violation_count}")
            
            with col4:
                if st.button('ğŸ‘ï¸ æŸ¥çœ‹', key=f'view_{report_id}'):
                    st.session_state[f'view_report_{report_id}'] = True
        
        for report_id, _, _, _, _, _ in reports:
            if st.session_state.get(f'view_report_{report_id}', False):
                st.markdown('---')
                full_report = get_report_by_id(report_id)
                if full_report:
                    st.json(full_report)
                    col1, col2 = st.columns(2)
                    with col1:
                        report_json = json.dumps(full_report, ensure_ascii=False, indent=2)
                        st.download_button(
                            label='ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š',
                            data=report_json,
                            file_name=f'report_{report_id}.json',
                            mime='application/json',
                            key=f'download_{report_id}'
                        )
                    with col2:
                        if st.button('ğŸ—‘ï¸ åˆ é™¤', key=f'delete_{report_id}'):
                            delete_report(report_id)
                            st.success('âœ… æŠ¥å‘Šå·²åˆ é™¤')
                            st.rerun()
                
                if st.button('å…³é—­è¯¦æƒ…', key=f'close_{report_id}'):
                    st.session_state[f'view_report_{report_id}'] = False
                    st.rerun()
    else:
        st.info('ğŸ“­ æš‚æ— æ£€æµ‹å†å²')

# ============ è§„åˆ™ç®¡ç†é¡µé¢ ============
elif page == 'è§„åˆ™ç®¡ç†':
    st.subheader('âš™ï¸ è§„åˆ™ç®¡ç†')
    
    rule_type = st.radio('é€‰æ‹©è§„åˆ™ç±»å‹', ('è¿è§„è§„åˆ™', 'æ•æ„Ÿè¯è§„åˆ™'))
    
    if rule_type == 'è¿è§„è§„åˆ™':
        file_path = Path('violation_rules.txt')
    else:
        file_path = Path('sensitive_words.txt')
    
    st.markdown(f'**ç¼–è¾‘: {file_path.name}**')
    
    if file_path.exists():
        current_content = file_path.read_text(encoding='utf-8')
    else:
        current_content = ''
    
    new_content = st.text_area('è§„åˆ™å†…å®¹ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰', value=current_content, height=400)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button('ğŸ’¾ ä¿å­˜ä¿®æ”¹'):
            file_path.write_text(new_content, encoding='utf-8')
            st.cache_data.clear()  # æ¸…é™¤ç¼“å­˜
            st.success(f'âœ… {file_path.name} å·²ä¿å­˜')
    
    with col2:
        if st.button('ğŸ”„ é‡æ–°åŠ è½½'):
            st.cache_data.clear()
            st.rerun()
    
    with col3:
        uploaded_rule_file = st.file_uploader('æˆ–ä¸Šä¼ è§„åˆ™æ–‡ä»¶', type=['txt'])
        if uploaded_rule_file is not None:
            content = uploaded_rule_file.read().decode('utf-8')
            file_path.write_text(content, encoding='utf-8')
            st.cache_data.clear()
            st.success(f'âœ… {file_path.name} å·²æ›´æ–°')
    
    rules = [line.strip() for line in new_content.split('\n') if line.strip()]
    st.info(f'ğŸ“Š å½“å‰å…±æœ‰ {len(rules)} æ¡è§„åˆ™')
    
    with st.expander('ğŸ“‹ è§„åˆ™é¢„è§ˆ'):
        for idx, rule in enumerate(rules[:20], 1):
            st.write(f"{idx}. {rule}")
        if len(rules) > 20:
            st.write(f"... è¿˜æœ‰ {len(rules) - 20} æ¡è§„åˆ™")

# é¡µè„š
st.markdown('---')
st.markdown('**åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ** | åŠŸèƒ½ï¼šæ£€æµ‹ | æ‰¹é‡æ£€æµ‹ | å†å²æŠ¥å‘Š | è§„åˆ™ç®¡ç†')
st.markdown('ğŸ’¡ æç¤º: ä¸ºåŠ é€Ÿé¦–æ¬¡åŠ è½½ï¼Œå·²é»˜è®¤å…³é—­è¯­ä¹‰æ£€æµ‹å’Œåˆ†ç±»å™¨ã€‚è¯·åœ¨éœ€è¦æ—¶å¯ç”¨ã€‚')
