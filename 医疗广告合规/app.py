import streamlit as st
import json
import time
from pathlib import Path
from datetime import datetime, timezone
import tempfile
import plotly.express as px
# åœ¨æœ€å¼€å§‹ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
@st.cache_resource
def get_semantic_model_cached():
    """ç¼“å­˜è¯­ä¹‰æ¨¡å‹ï¼Œä»…åŠ è½½ä¸€æ¬¡ã€‚"""
    from sentence_transformers import SentenceTransformer
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')


@st.cache_resource
def get_classifier_cached():
    """ç¼“å­˜åˆ†ç±»å™¨æ¨¡å‹ï¼Œä»…åŠ è½½ä¸€æ¬¡ã€‚"""
    try:
        from transformers import BertTokenizerFast, BertForSequenceClassification
        import torch
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        tokenizer = BertTokenizerFast.from_pretrained('models/violation_classifier')
        model = BertForSequenceClassification.from_pretrained('models/violation_classifier')
        model.to(device)
        model.eval()
        return {'model': model, 'tokenizer': tokenizer, 'device': device}
    except Exception:
        return None


@st.cache_data
def load_rules_cached(lang='ä¸­æ–‡'):
    """æŒ‰è¯­è¨€ç¼“å­˜è§„åˆ™æ–‡ä»¶åŠ è½½ã€‚æ”¯æŒ 'ä¸­æ–‡' ä¸ 'English'ï¼Œè‹¥è‹±æ–‡æ–‡ä»¶ä¸å­˜åœ¨åˆ™å›é€€åˆ°é»˜è®¤æ–‡ä»¶ã€‚"""
    from utils.text_processing import load_sensitive_words, load_violation_rules
    if lang == 'English':
        s_file = 'sensitive_words_en.txt'
        v_file = 'violation_rules_en.txt'
    else:
        s_file = 'sensitive_words.txt'
        v_file = 'violation_rules.txt'

    # å›é€€é€»è¾‘
    if not Path(s_file).exists():
        s_file = 'sensitive_words.txt'
    if not Path(v_file).exists():
        v_file = 'violation_rules.txt'

    return {
        'sensitive': load_sensitive_words(s_file),
        'violation': load_violation_rules(v_file)
    }


def highlight_text(text, matches):
    """å°†åŒ¹é…çš„ä½ç½®æ ‡çº¢ã€‚
    
    åŒ¹é…æ ¼å¼ï¼š[{'span': [start, end], 'matched_text': '...', ...}, ...]
    """
    if not matches:
        return text
    
    # æŒ‰èµ·å§‹ä½ç½®å€’åºæ’åˆ—ï¼Œä»åå¾€å‰æ›¿æ¢ä»¥é¿å…ä½ç½®åç§»
    sorted_matches = sorted(matches, key=lambda x: x['span'][0], reverse=True)
    
    result = text
    for match in sorted_matches:
        start, end = match['span']
        matched_text = match['matched_text']
        # ä½¿ç”¨éœ“è™¹æ ·å¼é«˜äº®ï¼ˆè¿è§„ä½¿ç”¨ç²‰è‰²ï¼Œæ•æ„Ÿè¯ä½¿ç”¨é’ç»¿è‰²ï¼‰
        # é»˜è®¤ä½¿ç”¨ç²‰è‰²é£æ ¼ï¼›è°ƒç”¨æ–¹å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–ç±»å
        highlighted = f'<span class="neon-match">{matched_text}</span>'
        result = result[:start] + highlighted + result[end:]
    
    return result


# å¯¼å…¥å…¶ä»–æ¨¡å—
from utils.text_processing import tokenize, regex_matches, violation_matches
from utils.database import init_db, save_report, get_reports, get_report_by_id, delete_report, get_statistics
import config

# ============ åˆå§‹åŒ– ============
st.set_page_config(page_title='åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ', layout='wide', initial_sidebar_state='expanded', page_icon='ğŸ›°ï¸')

# --- è§†è§‰æ ·å¼æ³¨å…¥ï¼ˆç§‘æŠ€æ„Ÿ / éœ“è™¹ + ç»ç’ƒæè´¨ï¼‰
st.markdown(
        """
        <style>
        :root{--bg1:#071021;--bg2:#081126;--accent:#00ffd5;--accent2:#7c3aed;--muted:#9fb3c8}
        /* è®©èƒŒæ™¯è¦†ç›–æ•´ä¸ªè§†å£å¹¶å›ºå®šï¼Œé¿å…æ»šåŠ¨å‡ºç°ç©ºç™½ */
        html, body, #root, [data-testid='stAppViewContainer']{height:100%; min-height:100vh; margin:0; padding:0}
        [data-testid='stAppViewContainer']{background: linear-gradient(135deg,var(--bg1),var(--bg2)) !important; background-attachment: fixed; color: #ffffff}
        .app-header{padding:18px;border-radius:12px;margin-bottom:12px; background: linear-gradient(90deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01)); box-shadow: 0 8px 30px rgba(2,6,23,0.7); backdrop-filter: blur(6px); border:1px solid rgba(255,255,255,0.03)}
        .app-title{font-size:28px; color: var(--accent); font-weight:700; letter-spacing:1px; text-shadow:0 0 18px rgba(0,255,213,0.08)}
        .app-sub{color:var(--muted); margin-top:4px}
        .card{background: linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01)); padding:14px; border-radius:10px; border:1px solid rgba(255,255,255,0.03)}
        .stButton>button {background: linear-gradient(90deg,var(--accent),var(--accent2)) !important; color: #04111a !important; font-weight:700; border-radius:8px !important; padding:8px 12px; box-shadow:0 6px 18px rgba(124,58,237,0.12)}
        textarea, input, .stTextInput>div>input {background: rgba(255,255,255,0.02) !important; color: #ffffff !important; border-radius:8px !important}
        .neon-match{background: rgba(255,20,147,0.10); color:#ffffff; font-weight:700; padding:2px 4px; border-radius:4px}
        .neon-sensitive{background: rgba(0,255,213,0.06); color:#ffffff; font-weight:700; padding:2px 4px; border-radius:4px}
        /* ä¾§è¾¹æ ï¼šé»˜è®¤æ‰€æœ‰æ–‡æœ¬ä¸ºç™½è‰²ï¼ˆä¸‹é¢è¦†ç›–è¯­è¨€æ ‡ç­¾ä¸ºé™é»˜è‰²ï¼‰ */
        [data-testid='stSidebar'] * { color: #ffffff !important; }
        .lang-exception .lang-label{ color: var(--muted) !important; font-weight:600; margin-bottom:6px; display:block }
        /* åªå°†ä¾§è¾¹æ ä¸­çš„ä¸‹æ‹‰/é€‰æ‹©æ¡†çš„é€‰ä¸­é¡¹æ–‡æœ¬è®¾ç½®ä¸ºé»‘è‰²ï¼ˆç›®æ ‡ï¼šè¯­è¨€é€‰æ‹©æ˜¾ç¤ºä¸ºé»‘è‰²ï¼‰ */
        [data-testid='stSidebar'] select { color: #000000 !important; }
        [data-testid='stSidebar'] div[role='combobox'] { color: #000000 !important; }
        [data-testid='stSidebar'] div[role='listbox'] > div { color: #000000 !important; }
        /* æå‡è¡¨å•æ§ä»¶å¯è¯»æ€§ */
        /* è¾“å…¥/æ–‡æœ¬åŸŸ placeholder é¢œè‰²æ›´äº®ä»¥æé«˜å¯¹æ¯”åº¦ */
        textarea::placeholder, input::placeholder, .stTextInput>div>input::placeholder { color: rgba(255,255,255,0.65) !important; }
        /* è¾“å…¥/æ–‡æœ¬åŸŸæ–‡æœ¬ä¿æŒç™½è‰²ï¼ŒèƒŒæ™¯ç¨å¾®åŠ æ·±æé«˜å¯¹æ¯” */
        textarea, input, .stTextInput>div>input { background: rgba(255,255,255,0.03) !important; color: #ffffff !important; border-radius:8px !important }
        /* Select / combobox æ˜¾ç¤ºé¡¹æ”¹ä¸ºé»‘å­—ï¼Œdropdown èƒŒæ™¯ä¸ºæµ…è‰²ä»¥æé«˜å¯è¯»æ€§ */
        [data-testid='stSidebar'] select, [data-testid='stSidebar'] div[role='combobox'] { background: rgba(255,255,255,0.95) !important; color: #000000 !important; border-radius:6px }
        /* radio/checkbox/slider æ ‡ç­¾ä½¿ç”¨ç™½è‰²æ–‡æœ¬ */
        [data-testid='stSidebar'] label, [data-testid='stSidebar'] .stMarkdown, [data-testid='stSidebar'] .stSlider { color: #ffffff !important }
        /* æŒ‰é’®æ–‡æœ¬ä½¿ç”¨ç™½è‰²ä»¥ä¾¿åœ¨æ·±è‰²èƒŒæ™¯ä¸Šå¯è¯» */
        .stButton>button { color: #ffffff !important }
        /* å¡ç‰‡ç‚«å½©è¾¹æ¡† */
        .fancy-card{padding:14px; border-radius:12px; background: linear-gradient(180deg, rgba(255,255,255,0.012), rgba(255,255,255,0.008)); border:1px solid rgba(255,255,255,0.03); box-shadow: 0 8px 30px rgba(2,6,23,0.6); transition: transform .18s ease, box-shadow .18s ease}
        .fancy-card:hover{transform: translateY(-6px); box-shadow: 0 18px 40px rgba(124,58,237,0.16);}
        .fancy-card .title{font-weight:700; color:var(--accent);}
        .fancy-card .meta{color:var(--muted); font-size:12px}
        /* ä¾§è¾¹æ è¦†ç›–æ•´é¡µé«˜åº¦ï¼Œè§†è§‰ä¸ä¸»èƒŒæ™¯ä¸€è‡´ */
        [data-testid='stSidebar']{background: linear-gradient(180deg, rgba(255,255,255,0.012), rgba(255,255,255,0.008)) !important; border-right:1px solid rgba(255,255,255,0.02); min-height:100vh; height:100vh; position:sticky; top:0}
        </style>

        <div class="app-header">
            <div class="app-title">ğŸ¥ åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ</div>
        </div>
        """,
        unsafe_allow_html=True,
)

# è¯­è¨€æ”¯æŒæ–‡æœ¬
_I18N = {
    'ä¸­æ–‡': {
        'detect':'æ£€æµ‹', 'batch':'æ‰¹é‡æ£€æµ‹', 'history':'å†å²æŠ¥å‘Š', 'rules':'è§„åˆ™ç®¡ç†',
        'title':'ğŸ¥ åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ', 'input_placeholder':'è¾“å…¥åŒ»ç–—å¹¿å‘Šæ–‡æ¡ˆ...', 'input':'ğŸ“ è¾“å…¥', 'results':'ğŸ“Š æ£€æµ‹ç»“æœ',
        'run_detect':'ğŸ” è¿è¡Œæ£€æµ‹', 'save':'ğŸ’¾ ä¿å­˜', 'download':'ğŸ“¥ ä¸‹è½½', 'no_history':'ğŸ“­ æš‚æ— æ£€æµ‹å†å²',
        'settings':'âš™ï¸ æ£€æµ‹é…ç½®', 'demo':'ğŸ›ï¸ æ¼”ç¤ºåŠ¨æ•ˆ',
        'select':'é€‰æ‹©åŠŸèƒ½', 'semantic_threshold':'è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼', 'enable_classifier':'å¯ç”¨å¾®è°ƒåˆ†ç±»å™¨', 'enable_violation_rules':'å¯ç”¨è¿è§„è§„åˆ™åŒ¹é…', 'enable_semantic':'å¯ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹',
        'performance':'â±ï¸ **æ€§èƒ½ä¼˜åŒ–**', 'performance_caption':'ä¸ºäº†åŠ é€Ÿé¦–æ¬¡åŠ è½½ï¼Œå·²é»˜è®¤å…³é—­è¯­ä¹‰æ£€æµ‹å’Œåˆ†ç±»å™¨ã€‚è¯·åœ¨éœ€è¦æ—¶å¯ç”¨ã€‚',
        'demo_detect':'ç¤ºä¾‹æ£€æµ‹æ•°', 'demo_violation':'ç¤ºä¾‹è¿è§„æ•°', 'demo_compliant':'ç¤ºä¾‹åˆè§„æ•°'
    },
    'English': {
        'detect':'Detect', 'batch':'Batch', 'history':'History', 'rules':'Rules',
        'title':'Medical Ad Compliance Checker', 'input_placeholder':'Enter advertisement text...', 'input':'ğŸ“ Input', 'results':'ğŸ“Š Results',
        'run_detect':'ğŸ” Run Detection', 'save':'ğŸ’¾ Save', 'download':'ğŸ“¥ Download', 'no_history':'No history yet',
        'settings':'âš™ï¸ Settings', 'demo':'ğŸ›ï¸ Demo Anim',
        'select':'Select', 'semantic_threshold':'Semantic similarity threshold', 'enable_classifier':'Enable fine-tuned classifier', 'enable_violation_rules':'Enable rule-based matching', 'enable_semantic':'Enable semantic similarity',
        'performance':'â±ï¸ **Performance**', 'performance_caption':'Semantic checks and classifier are off by default to speed up first load. Enable when needed.',
        'demo_detect':'Demo detections', 'demo_violation':'Demo violations', 'demo_compliant':'Demo compliant'
    }
}

def t(lang, key):
    return _I18N.get(lang, _I18N['ä¸­æ–‡']).get(key, key)

@st.cache_resource
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä»…ä¸€æ¬¡ï¼‰ã€‚"""
    init_db()
    return True

init_database()

# åœ¨é¡µé¢é¡¶éƒ¨ä½¿ç”¨ session state ä¸­çš„è¯­è¨€ï¼ˆè‹¥ä¾§è¾¹æ å°šæœªæ¸²æŸ“åˆ™å›é€€åˆ°ä¸­æ–‡ï¼‰
lang = st.session_state.get('language_select', 'ä¸­æ–‡')
st.title(t(lang, 'title'))
st.markdown('---')

# ============ ä¾§è¾¹æ  ============
with st.sidebar:
    # è¯­è¨€é€‰æ‹©å™¨ï¼ˆä¸­æ–‡ / Englishï¼‰ â€” ç‰¹æ®Šå¤„ç†ï¼šä¿ç•™ä¸ºé™é»˜é¢œè‰²
    st.markdown("<div class='lang-exception'><label class='lang-label'>è¯­è¨€ / Language</label></div>", unsafe_allow_html=True)
    lang = st.selectbox('è¯­è¨€ / Language', ('ä¸­æ–‡', 'English'), key='language_select', label_visibility='collapsed')

    # é¡µé¢é€‰é¡¹ï¼šä½¿ç”¨å†…éƒ¨ keys å¹¶æ˜¾ç¤ºæœ¬åœ°åŒ–æ ‡ç­¾
    pages = {
        'detect': t(lang, 'detect'),
        'batch': t(lang, 'batch'),
        'history': t(lang, 'history'),
        'rules': t(lang, 'rules')
    }

    page = st.radio(t(lang, 'select'), options=list(pages.keys()), format_func=lambda k: pages[k])

    if page != 'rules':
        st.header(t(lang, 'settings'))
        semantic_threshold = st.slider(t(lang, 'semantic_threshold'), 0.0, 1.0, config.SEMANTIC_THRESHOLD, 0.01)
        enable_classifier = st.checkbox(t(lang, 'enable_classifier'), value=False)  # é»˜è®¤å…³é—­ä»¥åŠ é€Ÿ
        enable_violation_rules = st.checkbox(t(lang, 'enable_violation_rules'), value=True)
        enable_semantic = st.checkbox(t(lang, 'enable_semantic'), value=False)  # é»˜è®¤å…³é—­ä»¥åŠ é€Ÿ
    # åŠ¨æ•ˆæ¼”ç¤ºæŒ‰é’®ï¼ˆæœ¬åœ°åŒ–ï¼‰
    if st.button(t(lang, 'demo')):
        def run_animation():
            demo_cols = st.columns(3)
            t1 = demo_cols[0].empty()
            t2 = demo_cols[1].empty()
            t3 = demo_cols[2].empty()
            # æ¼”ç¤ºè®¡æ•°åŠ¨ç”»
            for i in range(0, 101, 5):
                t1.metric(t(lang, 'demo_detect'), f'{i}')
                t2.metric(t(lang, 'demo_violation'), f'{int(i*0.35)}')
                t3.metric(t(lang, 'demo_compliant'), f'{int(i*0.65)}')
                time.sleep(0.04)
            # æ˜¾ç¤ºçŸ­æš‚æ‰«ææ¡
            scan = st.empty()
            scan.markdown("<div class='scanner' style='height:6px;border-radius:6px;margin-top:8px;'></div>", unsafe_allow_html=True)
            time.sleep(1.2)
            scan.empty()
        run_animation()

    st.markdown('---')
    st.markdown(t(lang, 'performance'))
    st.caption(t(lang, 'performance_caption'))


# ============ æ£€æµ‹é¡µé¢ ============
if page == 'detect':
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader(t(lang, 'input'))
        input_text = st.text_area(t(lang, 'input'), height=300, placeholder=t(lang, 'input_placeholder'))

    with col2:
        st.subheader(t(lang, 'results'))
        perform_detection = False
        if input_text:
            perform_detection = st.button(t(lang, 'run_detect'), key='text_detect_btn')
        
        if perform_detection and input_text:
            with st.spinner('æ­£åœ¨æ£€æµ‹...'):
                # 1. åˆ†è¯
                st.info('ğŸ“Š æ‰§è¡Œåˆ†è¯...')
                tokens = tokenize(input_text)
                
                # 2. åŠ è½½è§„åˆ™ï¼ˆæŒ‰è¯­è¨€ï¼‰
                rules_data = load_rules_cached(lang=lang)
                # å¦‚æœä¸¤ç±»è§„åˆ™å‡ä¸ºç©ºï¼Œæç¤ºå¯èƒ½çš„æ–‡ä»¶è·¯å¾„é—®é¢˜
                if not rules_data.get('sensitive') and not rules_data.get('violation'):
                    proj_root = Path(__file__).resolve().parent
                    st.warning(f'âš ï¸ æœªåŠ è½½åˆ°è§„åˆ™æ–‡ä»¶ï¼ˆæˆ–æ–‡ä»¶ä¸ºç©ºï¼‰ã€‚è¯·ç¡®è®¤ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«è§„åˆ™ï¼š\n- {proj_root / "violation_rules.txt"}\n- {proj_root / "sensitive_words.txt"}')
                
                # 3. æ•æ„Ÿè¯æ­£åˆ™åŒ¹é…
                st.info('ğŸ” æ‰§è¡Œæ•æ„Ÿè¯åŒ¹é…...')
                regex_res = regex_matches(input_text, rules_data['sensitive'], flags=config.REGEX_FLAGS)
                
                # 4. è¿è§„è§„åˆ™åŒ¹é…
                violation_res = []
                if enable_violation_rules:
                    st.info('ğŸ“‹ æ‰§è¡Œè¿è§„è§„åˆ™åŒ¹é…...')
                    violation_res = violation_matches(input_text, rules=rules_data['violation'])
                
                # 5. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹
                semantic_res = []
                if enable_semantic:
                    st.info('â³ åŠ è½½è¯­ä¹‰æ¨¡å‹ä¸­ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼‰...')
                    try:
                        model = get_semantic_model_cached()
                        st.info('ğŸ”„ æ‰§è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹...')
                        from numpy import argmax
                        from sklearn.metrics.pairwise import cosine_similarity
                        texts = [input_text] + rules_data['sensitive']
                        embs = model.encode(texts, convert_to_numpy=True)
                        sims = cosine_similarity(embs[0:1], embs[1:])[0]
                        for i, s in enumerate(sims):
                            if s >= semantic_threshold:
                                semantic_res.append({'example': rules_data['sensitive'][i], 'score': float(s)})
                        semantic_res = sorted(semantic_res, key=lambda x: x['score'], reverse=True)[:3]
                    except Exception as e:
                        st.warning(f'âš ï¸ è¯­ä¹‰æ£€æµ‹å¤±è´¥: {e}')
                
                # 6. åˆ†ç±»å™¨é¢„æµ‹
                classifier_res = None
                if enable_classifier:
                    st.info('â³ åŠ è½½åˆ†ç±»å™¨ä¸­ï¼ˆé¦–æ¬¡è¾ƒæ…¢ï¼‰...')
                    try:
                        import torch
                        import numpy as np
                        clf = get_classifier_cached()
                        if clf:
                            tokenizer, model, device = clf['tokenizer'], clf['model'], clf['device']
                            inputs = tokenizer(input_text, truncation=True, padding='max_length', max_length=256, return_tensors='pt')
                            inputs = {k: v.to(device) for k, v in inputs.items()}
                            with torch.no_grad():
                                outputs = model(**inputs)
                                logits = outputs.logits.cpu().numpy()[0]
                                e_x = np.exp(logits - np.max(logits))
                                probs = e_x / e_x.sum()
                                label = int(argmax(probs))
                                classifier_res = {'label': label, 'score': float(probs[label]), 'probs': [float(p) for p in probs]}
                    except Exception as e:
                        st.warning(f'âš ï¸ åˆ†ç±»å™¨åŠ è½½å¤±è´¥: {e}')
                
                # 7. ç»¼åˆåˆ¤å®š
                has_violations = len(violation_res) > 0 or len(regex_res) > 0 or len(semantic_res) > 0
                if classifier_res and classifier_res.get('label') == 1:
                    has_violations = True
                
                verdict = 'åˆè§„' if not has_violations else 'ç–‘ä¼¼è¿è§„'
                verdict_display = 'ğŸŸ¢ åˆè§„' if not has_violations else 'ğŸ”´ ç–‘ä¼¼è¿è§„'
                
                st.markdown(f"### ç»¼åˆåˆ¤å®š: {verdict_display}")
                
                # ç»“æœæ ‡ç­¾é¡µ
                tab1, tab2, tab3, tab4, tab5 = st.tabs(['è¿è§„è§„åˆ™åŒ¹é…', 'æ•æ„Ÿè¯åŒ¹é…', 'è¯­ä¹‰ç›¸ä¼¼åº¦', 'åˆ†ç±»å™¨', 'åˆ†è¯ç»“æœ'])
                
                with tab1:
                    if enable_violation_rules and violation_res:
                        st.markdown(f"**å‘ç° {len(violation_res)} ä¸ªè¿è§„è§„åˆ™åŒ¹é…**")
                        st.markdown('---')
                        st.markdown('**æ£€æµ‹æ–‡æœ¬ï¼ˆçº¢è‰²æ ‡è®°ä¸ºé—®é¢˜ä½ç½®ï¼‰ï¼š**')
                        highlighted = highlight_text(input_text, violation_res)
                        st.markdown(highlighted, unsafe_allow_html=True)
                        st.markdown('---')
                        st.markdown('**è¯¦ç»†åŒ¹é…ï¼š**')
                        for idx, match in enumerate(violation_res, 1):
                            st.write(f"**{idx}. è¿è§„è¯**: `{match['rule']}`")
                            st.divider()
                    else:
                        st.info('âœ… æœªæ£€æµ‹åˆ°è¿è§„è§„åˆ™åŒ¹é…')
                
                with tab2:
                    if regex_res:
                        st.markdown(f"**å‘ç° {len(regex_res)} ä¸ªæ•æ„Ÿè¯åŒ¹é…**")
                        st.markdown('---')
                        st.markdown('**æ£€æµ‹æ–‡æœ¬ï¼ˆé’è‰²æ ‡è®°ä¸ºæ•æ„Ÿè¯ï¼‰ï¼š**')
                        highlighted_sensitive = highlight_text(input_text, regex_res)
                        st.markdown(highlighted_sensitive, unsafe_allow_html=True)
                        st.markdown('---')
                        st.markdown('**è¯¦ç»†åŒ¹é…ï¼š**')
                        for idx, match in enumerate(regex_res, 1):
                            st.write(f"**{idx}. æ•æ„Ÿè¯**: `{match['matched_text']}`")
                            st.divider()
                    else:
                        st.info('âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯')
                
                with tab3:
                    if enable_semantic and semantic_res:
                        st.markdown(f"**å‘ç° {len(semantic_res)} ä¸ªç›¸ä¼¼åº¦é«˜çš„ç¤ºä¾‹**")
                        for idx, match in enumerate(semantic_res, 1):
                            st.write(f"**{idx}. ç¤ºä¾‹**: `{match['example']}`")
                            st.write(f"   **ç›¸ä¼¼åº¦åˆ†æ•°**: {match['score']:.4f}")
                            st.divider()
                    elif enable_semantic:
                        st.info('âœ… æœªæ£€æµ‹åˆ°é«˜ç›¸ä¼¼åº¦ç¤ºä¾‹')
                    else:
                        st.info('â­ï¸ è¯­ä¹‰æ£€æµ‹å·²ç¦ç”¨')
                
                with tab4:
                    if classifier_res:
                        label_text = 'ç–‘ä¼¼è¿è§„ (label=1)' if classifier_res['label'] == 1 else 'åˆè§„ (label=0)'
                        st.write(f"**åˆ†ç±»ç»“æœ**: {label_text}")
                        st.write(f"**ç½®ä¿¡åº¦**: {classifier_res['score']:.4f}")
                        col_prob1, col_prob2 = st.columns(2)
                        with col_prob1:
                            st.metric('åˆè§„æ¦‚ç‡', f"{classifier_res['probs'][0]:.4f}")
                        with col_prob2:
                            st.metric('è¿è§„æ¦‚ç‡', f"{classifier_res['probs'][1]:.4f}")
                    else:
                        st.info('â­ï¸ åˆ†ç±»å™¨æœªå¯ç”¨æˆ–åŠ è½½å¤±è´¥')
                
                with tab5:
                    st.write('**åˆ†è¯ç»“æœ**')
                    tokens_str = ' / '.join(tokens)
                    st.text(tokens_str)
                    st.write(f'**æ€»è¯æ•°**: {len(tokens)}')
                
                # ç”ŸæˆæŠ¥å‘Š
                st.markdown('---')
                report_data = {
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'input_text': input_text,
                    'ocr_text': '',
                    'tokens': tokens,
                    'regex_matches': regex_res,
                    'violation_rule_matches': violation_res,
                    'semantic_matches': semantic_res,
                    'classifier': classifier_res,
                    'verdict': verdict
                }
                
                col_save, col_download = st.columns(2)
                with col_save:
                    if st.button('ğŸ’¾ ä¿å­˜åˆ°å†å²'):
                        save_report(report_data)
                        st.success('âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°æ•°æ®åº“')
                
                with col_download:
                    report_json = json.dumps(report_data, ensure_ascii=False, indent=2)
                    st.download_button(
                        label='ğŸ“¥ ä¸‹è½½æŠ¥å‘Š (JSON)',
                        data=report_json,
                        file_name=f'report_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}.json',
                        mime='application/json'
                    )

# ============ æ‰¹é‡æ£€æµ‹é¡µé¢ ============
elif page == 'batch':
    st.subheader('ğŸ“¦ æ‰¹é‡æ£€æµ‹')
    st.markdown('**ä¸Šä¼ åŒ…å«æ–‡æœ¬çš„æ–‡ä»¶è¿›è¡Œæ‰¹é‡æ£€æµ‹**')
    
    uploaded_file = st.file_uploader('é€‰æ‹©æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰', type=['txt', 'csv'])
    
    if uploaded_file is not None:
        try:
            content = uploaded_file.read().decode('utf-8')
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            st.info(f'ğŸ“Š æ£€æµ‹åˆ° {len(lines)} æ¡æ–‡æœ¬')
            
            if st.button('ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹'):
                results = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                rules_data = load_rules_cached(lang=lang)
                
                for idx, text in enumerate(lines):
                    status_text.text(f'å¤„ç†ä¸­ {idx + 1}/{len(lines)}...')
                    
                    try:
                        regex_res = regex_matches(text, rules_data['sensitive'])
                        violation_res = []
                        if enable_violation_rules:
                            violation_res = violation_matches(text, rules=rules_data['violation'])
                        
                        has_violations = len(violation_res) > 0 or len(regex_res) > 0
                        verdict = 'ç–‘ä¼¼è¿è§„' if has_violations else 'åˆè§„'
                        
                        results.append({
                            'text': text[:100],
                            'verdict': verdict,
                            'violation_count': len(violation_res) + len(regex_res)
                        })
                    except Exception:
                        results.append({
                            'text': text[:100],
                            'verdict': 'é”™è¯¯',
                            'violation_count': 0
                        })
                    
                    progress_bar.progress((idx + 1) / len(lines))
                
                status_text.empty()
                st.success(f'âœ… æ‰¹é‡æ£€æµ‹å®Œæˆ')
                
                st.subheader('æ£€æµ‹ç»“æœ')
                st.dataframe(results)
                
                compliant = sum(1 for r in results if r['verdict'] == 'åˆè§„')
                violations = sum(1 for r in results if r['verdict'] == 'ç–‘ä¼¼è¿è§„')
                
                col1, col2, col3 = st.columns(3)
                col1.metric('æ€»æ•°', len(results))
                col2.metric('åˆè§„', compliant)
                col3.metric('è¿è§„', violations)
                
                result_json = json.dumps(results, ensure_ascii=False, indent=2)
                st.download_button(
                    label='ğŸ“¥ ä¸‹è½½æ‰¹é‡ç»“æœ (JSON)',
                    data=result_json,
                    file_name=f'batch_results_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}.json',
                    mime='application/json'
                )
        except Exception as e:
            st.error(f'âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}')

# ============ å†å²æŠ¥å‘Šé¡µé¢ ============
elif page == 'history':
    st.subheader('ğŸ“œ æ£€æµ‹å†å²')
    
    stats = get_statistics()
    c1, c2, c3 = st.columns(3)
    c1.metric('æ€»æ£€æµ‹æ•°', stats['total'])
    c2.metric('è¿è§„æ•°', stats['violations'])
    c3.metric('åˆè§„æ•°', stats['compliant'])

    # å¯è§†åŒ–ï¼šæŒ‰åˆ¤å®šç»˜åˆ¶æŸ±çŠ¶å›¾
    st.markdown('---')
    reports = get_reports(limit=100)
    verdicts = ['åˆè§„', 'ç–‘ä¼¼è¿è§„']
    counts = [stats.get('compliant', 0), stats.get('violations', 0)]
    fig = px.bar(x=verdicts, y=counts, color=verdicts, color_discrete_map={
        'åˆè§„':'#00ffd5', 'ç–‘ä¼¼è¿è§„':'#ff4d9e'
    }, labels={'x':'åˆ¤å®š','y':'æ•°é‡'}, title='æ£€æµ‹åˆ¤å®šåˆ†å¸ƒ')
    fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)', font_color='#cfe9f8')
    st.plotly_chart(fig, use_container_width=True)

    st.markdown('---')
    if reports:
        st.markdown(f'**æ˜¾ç¤ºæœ€è¿‘ {len(reports)} æ¡è®°å½•ï¼ˆå¡ç‰‡è§†å›¾ï¼‰**')
        # ä»¥ä¸¤åˆ—æ’ç‰ˆå¡ç‰‡
        for i in range(0, len(reports), 2):
            row = reports[i:i+2]
            cols = st.columns(len(row))
            for col, (report_id, timestamp, input_text, verdict, violation_count, created_at) in zip(cols, row):
                with col:
                    icon = 'ğŸŸ¢' if verdict == 'åˆè§„' else 'ğŸ”´'
                    html = f"""
                    <div class='fancy-card'>
                      <div class='title'>{icon} {input_text[:120]}</div>
                      <div class='meta'>{timestamp} Â· è¿è§„æ•°: <strong style='color:#ff4d9e'>{violation_count}</strong></div>
                      <div style='margin-top:8px;color:var(--muted)'>åˆ¤å®šï¼š<strong>{verdict}</strong></div>
                    </div>
                    """
                    st.markdown(html, unsafe_allow_html=True)
                    # æ“ä½œæŒ‰é’®
                    full_report = get_report_by_id(report_id)
                    if full_report:
                        report_json = json.dumps(full_report, ensure_ascii=False, indent=2)
                    else:
                        report_json = '{}'
                    btn_col1, btn_col2 = st.columns([1,1])
                    with btn_col1:
                        st.download_button('ğŸ“¥ ä¸‹è½½', report_json, f'report_{report_id}.json', 'application/json', key=f'dl_{report_id}')
                    with btn_col2:
                        if st.button('ğŸ—‘ï¸ åˆ é™¤', key=f'del_{report_id}'):
                            delete_report(report_id)
                            st.experimental_rerun()
    else:
        st.info('ğŸ“­ æš‚æ— æ£€æµ‹å†å²')

# ============ è§„åˆ™ç®¡ç†é¡µé¢ ============
elif page == 'rules':
    st.subheader('âš™ï¸ è§„åˆ™ç®¡ç†')
    
    rule_type = st.radio('é€‰æ‹©è§„åˆ™ç±»å‹', ('è¿è§„è§„åˆ™', 'æ•æ„Ÿè¯è§„åˆ™'))
    
    if rule_type == 'è¿è§„è§„åˆ™':
        file_path = Path('violation_rules.txt')
    else:
        file_path = Path('sensitive_words.txt')
    
    st.markdown(f'**ç¼–è¾‘: {file_path.name}**')
    
    if file_path.exists():
        current_content = file_path.read_text(encoding='utf-8')
    else:
        current_content = ''
    
    # è¯Šæ–­ä¸æ¢å¤å·¥å…·
    st.markdown('**è¯Šæ–­ä¸æ¢å¤**')
    diag_col1, diag_col2 = st.columns([1, 1])
    with diag_col1:
        if st.button('ğŸ”§ è¿è¡Œè¯Šæ–­'):
            msgs = []
            for fname in ['violation_rules.txt', 'sensitive_words.txt']:
                p = Path(fname)
                if p.exists():
                    size = p.stat().st_size
                    if size > 0:
                        msgs.append(f'{fname}: å­˜åœ¨ ({size} bytes)')
                    else:
                        msgs.append(f'{fname}: å­˜åœ¨ï¼Œä½†æ–‡ä»¶ä¸ºç©º')
                else:
                    msgs.append(f'{fname}: ä¸å­˜åœ¨')

            st.info('\n'.join(msgs))

    with diag_col2:
        if st.button('ğŸ”„ æ¢å¤é»˜è®¤è§„åˆ™'):
            # é»˜è®¤è§„åˆ™æ ·æœ¬ï¼ˆç®€çŸ­ç‰ˆï¼‰
            default_violation = '''å›½å®¶çº§
ä¸–ç•Œçº§
é¦–é€‰
åŒ…æ²»ç™¾ç—…
æ— å‰¯ä½œç”¨
ç™¾åˆ†ä¹‹ç™¾
ä¿è¯æ²»æ„ˆ
ç‰¹æ•ˆ
é€Ÿæ•ˆ
æ°¸ä¹…''' 
            default_sensitive = '''ç™¾åˆ†ä¹‹ç™¾
ä¿è¯æ²»æ„ˆ
æ— å‰¯ä½œç”¨
å¿«é€Ÿæ²»æ„ˆ
ç«‹ç«¿è§å½±
å›½å®¶è®¤å¯
æƒå¨è¯å®
é›¶é£é™©
å”¯ä¸€ç–—æ³•
é•¿æœŸå®‰å…¨'''
            try:
                Path('violation_rules.txt').write_text(default_violation, encoding='utf-8')
                Path('sensitive_words.txt').write_text(default_sensitive, encoding='utf-8')
                st.cache_data.clear()
                st.success('âœ… å·²æ¢å¤é»˜è®¤è§„åˆ™åˆ° violation_rules.txt ä¸ sensitive_words.txt')
                # æ›´æ–° current_content ä»¥åœ¨ç¼–è¾‘å™¨ä¸­æ˜¾ç¤º
                current_content = Path(file_path).read_text(encoding='utf-8')
            except Exception as e:
                st.error(f'æ¢å¤å¤±è´¥: {e}')

    new_content = st.text_area('è§„åˆ™å†…å®¹ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰', value=current_content, height=400)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button('ğŸ’¾ ä¿å­˜ä¿®æ”¹'):
            file_path.write_text(new_content, encoding='utf-8')
            st.cache_data.clear()  # æ¸…é™¤ç¼“å­˜
            st.success(f'âœ… {file_path.name} å·²ä¿å­˜')
    
    with col2:
        if st.button('ğŸ”„ é‡æ–°åŠ è½½'):
            st.cache_data.clear()
            st.rerun()
    
    with col3:
        uploaded_rule_file = st.file_uploader('æˆ–ä¸Šä¼ è§„åˆ™æ–‡ä»¶', type=['txt'])
        if uploaded_rule_file is not None:
            content = uploaded_rule_file.read().decode('utf-8')
            file_path.write_text(content, encoding='utf-8')
            st.cache_data.clear()
            st.success(f'âœ… {file_path.name} å·²æ›´æ–°')
    
    rules = [line.strip() for line in new_content.split('\n') if line.strip()]
    st.info(f'ğŸ“Š å½“å‰å…±æœ‰ {len(rules)} æ¡è§„åˆ™')
    
    with st.expander('ğŸ“‹ è§„åˆ™é¢„è§ˆ'):
        for idx, rule in enumerate(rules[:20], 1):
            st.write(f"{idx}. {rule}")
        if len(rules) > 20:
            st.write(f"... è¿˜æœ‰ {len(rules) - 20} æ¡è§„åˆ™")

# é¡µè„š
st.markdown('---')
st.markdown('**åŒ»ç–—å¹¿å‘Šåˆè§„æ£€æµ‹ç³»ç»Ÿ** | åŠŸèƒ½ï¼šæ£€æµ‹ | æ‰¹é‡æ£€æµ‹ | å†å²æŠ¥å‘Š | è§„åˆ™ç®¡ç†')
st.markdown('ğŸ’¡ æç¤º: ä¸ºåŠ é€Ÿé¦–æ¬¡åŠ è½½ï¼Œå·²é»˜è®¤å…³é—­è¯­ä¹‰æ£€æµ‹å’Œåˆ†ç±»å™¨ã€‚è¯·åœ¨éœ€è¦æ—¶å¯ç”¨ã€‚')
